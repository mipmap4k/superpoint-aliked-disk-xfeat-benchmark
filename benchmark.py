import os
import cv2
import numpy as np
import pandas as pd
import torch
import gc
import time
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
matplotlib.use('Agg')

import seaborn as sns
from dataclasses import dataclass
from typing import List, Tuple, Dict
from lightglue import SuperPoint, ALIKED, DISK, LightGlue
from sklearn.metrics import average_precision_score

# ==========================================
# 1. –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•
# ==========================================

@dataclass
class KeypointData:
    x: float
    y: float
    confidence: float

@dataclass
class RawMatch:
    point_sat: KeypointData
    point_drone: KeypointData
    score: float = 1.0 

# ==========================================
# 2. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
# ==========================================

class ResultVisualizer:
    def __init__(self, base_output_dir="vis_results", kp_scale_factor=40):
        self.base_output_dir = base_output_dir
        self.kp_scale_factor = kp_scale_factor
        if not os.path.exists(self.base_output_dir):
            os.makedirs(self.base_output_dir)

    def _draw_on_image(self, img, all_kpts: List[KeypointData], inlier_kpts: List[KeypointData], draw_all=True):
        vis = img.copy()
        # 1. –°–ø—É—Ç–Ω–∏–∫: –ö—Ä–∞—Å–Ω—ã–µ (–≤—Å–µ) + –ó–µ–ª–µ–Ω—ã–µ (–∏–Ω–ª–∞–π–µ—Ä—ã)
        if draw_all:
            for kp in all_kpts:
                r = max(1, min(int(kp.confidence * self.kp_scale_factor), 12))
                cv2.circle(vis, (int(kp.x), int(kp.y)), r, (255, 0, 0), -1)
        
        # 2. –ò–Ω–ª–∞–π–µ—Ä—ã (–ó–µ–ª–µ–Ω—ã–º) - —Ä–∏—Å—É—é—Ç—Å—è –≤—Å–µ–≥–¥–∞ –ø–æ–≤–µ—Ä—Ö
        for kp in inlier_kpts:
            r = max(1, min(int(kp.confidence * self.kp_scale_factor), 12))
            cv2.circle(vis, (int(kp.x), int(kp.y)), r, (0, 255, 0), -1)
        return vis

    def save_analysis_plot(self, img_s, img_d, all_sat_kpts, raw_matches, inliers, model_name, frame_name):
        save_path = os.path.join(self.base_output_dir, model_name)
        os.makedirs(save_path, exist_ok=True)

        inliers_sat = [m.point_sat for m in inliers]
        inliers_drone = [m.point_drone for m in inliers]

        vis_s = self._draw_on_image(img_s, all_sat_kpts, inliers_sat, draw_all=True)
        vis_d = self._draw_on_image(img_d, [], inliers_drone, draw_all=False) # –¢–æ–ª—å–∫–æ –∑–µ–ª–µ–Ω—ã–µ

        fig = plt.figure(figsize=(24, 12))
        gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.8])

        ax0 = fig.add_subplot(gs[:, 0]); ax0.imshow(vis_s); ax0.set_title("Satellite Map (All + Inliers)"); ax0.axis('off')
        ax1 = fig.add_subplot(gs[:, 1]); ax1.imshow(vis_d); ax1.set_title("Drone Image (Inliers Only)"); ax1.axis('off')

        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ Confidence
        ax2 = fig.add_subplot(gs[0, 2])
        raw_confs = [m.point_sat.confidence for m in raw_matches]
        inl_confs = [m.point_sat.confidence for m in inliers]
        if raw_confs:
            ax2.hist(raw_confs, bins=25, alpha=0.4, color='red', label='Raw Matches')
            if inl_confs: ax2.hist(inl_confs, bins=25, alpha=0.7, color='green', label='Inliers')
            ax2.set_title("Confidence Distribution")
            ax2.set_xlabel("Confidence Value"); ax2.set_ylabel("Frequency"); ax2.legend()
        ax2.grid(True, alpha=0.2)

        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ Scores
        ax3 = fig.add_subplot(gs[1, 2])
        raw_scores = [m.score for m in raw_matches]
        inl_scores = [m.score for m in inliers]
        if raw_scores:
            ax3.hist(raw_scores, bins=25, alpha=0.4, color='blue', label='Raw Scores')
            if inl_scores: ax3.hist(inl_scores, bins=25, alpha=0.7, color='cyan', label='Inlier Scores')
            ax3.set_title("Matching Scores Distribution")
            ax3.set_xlabel("Matching Score"); ax3.set_ylabel("Frequency"); ax3.legend()
        ax3.grid(True, alpha=0.2)

        plt.suptitle(f"Model: {model_name} | {frame_name} | Inliers: {len(inliers)}", fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(save_path, f"{frame_name}.png"), dpi=100)
        plt.close()

    def save_summary_statistics(self, all_model_results: Dict[str, List[Dict]]):
        if not all_model_results: return
        save_path = os.path.join(self.base_output_dir, "summary_report")
        os.makedirs(save_path, exist_ok=True)
        
        data_list = []
        for model_name, frames in all_model_results.items():
            for f in frames:
                data_list.append({'Model': model_name.upper(), 'mAP': f['mAP'], 'Inliers': f['inliers']})
        df = pd.DataFrame(data_list)

        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        sns.boxplot(x='Model', y='mAP', data=df, ax=axes[0], palette="Set2")
        axes[0].set_title("Stability: mAP Distribution")
        sns.boxplot(x='Model', y='Inliers', data=df, ax=axes[1], palette="Set3", hue='Model', legend=False)
        axes[1].set_title("Stability: Inlier Count")
        
        plt.savefig(os.path.join(save_path, "global_comparison.png"))
        print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {save_path}")
        plt.close()

# ==========================================
# 3. –ú–û–î–£–õ–ò –û–ë–†–ê–ë–û–¢–ö–ò
# ==========================================

class GeometryVerifier:
    def __init__(self, threshold=3.0):
        self.threshold = threshold

    def filter_matches(self, matches: List[RawMatch]) -> Tuple[List[RawMatch], List[RawMatch]]:
        if len(matches) < 4: return [], matches
        pts_s = np.array([[m.point_sat.x, m.point_sat.y] for m in matches], dtype=np.float32)
        pts_d = np.array([[m.point_drone.x, m.point_drone.y] for m in matches], dtype=np.float32)
        H, mask = cv2.findHomography(pts_s, pts_d, cv2.USAC_MAGSAC, self.threshold)
        inliers, outliers = [], []
        if mask is not None:
            mask = mask.ravel()
            for i, m in enumerate(matches):
                if mask[i]: inliers.append(m)
                else: outliers.append(m)
        return inliers, outliers

class MetricsCalculator:
    def calculate_reliability_metrics(self, all_sat_kpts: List[KeypointData], inliers: List[RawMatch]):
        if not all_sat_kpts: return {"mAP": 0.0, "inliers": 0}
        inlier_coords = set([(m.point_sat.x, m.point_sat.y) for m in inliers])
        y_true, y_scores = [], []
        for kp in all_sat_kpts:
            y_scores.append(kp.confidence)
            y_true.append(1 if (kp.x, kp.y) in inlier_coords else 0)
        ap = average_precision_score(y_true, y_scores) if sum(y_true) > 0 else 0
        return {"mAP": ap, "inliers": len(inliers)}
    
class ImagePreprocessor:
    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):
        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)

    def __call__(self, img):
        # 1. –ü–µ—Ä–µ–≤–æ–¥ –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        # 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ (CLAHE)
        # –≠—Ç–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π –Ω–∞ –±–µ—Ç–æ–Ω–µ/–∑–µ–º–ª–µ
        gray = self.clahe.apply(gray)

        # 3. –†–∞—Å—Ç—è–∂–µ–Ω–∏–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã (–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–∞–º—ã–π —Ç–µ–º–Ω—ã–π –ø–∏–∫—Å–µ–ª—å = 0, —Å–∞–º—ã–π —Å–≤–µ—Ç–ª—ã–π = 255
        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX)

        # 4. –ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ 3-–∫–∞–Ω–∞–ª—å–Ω–æ–µ "–ø—Å–µ–≤–¥–æ-RGB"
        # –ù—É–∂–Ω–æ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ (SuperPoint/XFeat) –Ω–µ —Ä—É–≥–∞–ª–∏—Å—å –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤
        img_pseudo_rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)

        # 5. –õ–µ–≥–∫–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ —à—É–º–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        # –ü–æ–º–æ–≥–∞–µ—Ç, –µ—Å–ª–∏ –Ω–∞ —Ñ–æ—Ç–æ –¥—Ä–æ–Ω–∞ –µ—Å—Ç—å "–∑–µ—Ä–Ω–æ" (ISO —à—É–º)
        img_pseudo_rgb = cv2.GaussianBlur(img_pseudo_rgb, (3, 3), 0)

        return img_pseudo_rgb

# ==========================================
# 4. –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ë–ï–ù–ß–ú–ê–†–ö
# ==========================================

class WaypointBenchmark:
    def __init__(self, device='cpu', vis_scale=40):
        self.device = device
        self.visualizer = ResultVisualizer(kp_scale_factor=vis_scale)
        self.configs = {"xfeat": "xfeat", "superpoint": "superpoint", "aliked": "aliked", "disk": "disk"}

    def execute(self, provider, verifier, metrics_calc, target_width=1024):
        preprocessor = ImagePreprocessor()
        final_summary = {}
        all_frames_data = {}

        for model_name, match_cfg in self.configs.items():
            print(f"\nüöÄ –ó–∞–ø—É—Å–∫: {model_name.upper()}")
            
            if model_name == "xfeat":
                model = torch.hub.load('verlab/accelerated_features', 'XFeat', pretrained=True, top_k=1024).to(self.device).eval()
            else:
                from lightglue import SuperPoint, ALIKED, DISK
                ext_class = {"superpoint": SuperPoint, "aliked": ALIKED, "disk": DISK}[model_name]
                extractor = ext_class(max_num_keypoints=1024).to(self.device).eval()
                matcher = LightGlue(features=match_cfg).to(self.device).eval()
            
            model_frame_stats = []
            frame_idx = 0
            
            for img_s_raw, img_d_raw, meta in provider.generator():
                if frame_idx == 5: break

                frame_idx += 1

                img_name = meta.get('file', f'frame_{frame_idx}').split('.')[0]
                
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                h, w = img_s_raw.shape[:2]
                scale = target_width / w
                img_s = cv2.resize(img_s_raw, (target_width, int(h * scale)))
                img_d = cv2.resize(img_d_raw, (target_width, int(h * scale)))

                img_s = preprocessor(img_s)
                img_d = preprocessor(img_d)

                img_s_t = torch.from_numpy(img_s).float().permute(2, 0, 1).unsqueeze(0).to(self.device) / 255.0
                img_d_t = torch.from_numpy(img_d).float().permute(2, 0, 1).unsqueeze(0).to(self.device) / 255.0
            
                with torch.inference_mode():
                    start_t = time.perf_counter()
                    if model_name == "xfeat":
                        f0 = model.detectAndCompute(img_s_t, top_k=1024)[0]
                        f1 = model.detectAndCompute(img_d_t, top_k=1024)[0]

                        f0['image_size'] = (img_s_t.shape[-1], img_s_t.shape[-2])
                        f1['image_size'] = (img_d_t.shape[-1], img_d_t.shape[-2])
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è XFeat
                        if f0['keypoints'].shape[0] == 0 or f1['keypoints'].shape[0] == 0:
                            print(f"\r  [{frame_idx}] ‚ö†Ô∏è {img_name} | –¢–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–ø—É—Å–∫...", end='')
                            model_frame_stats.append({"mAP": 0.0, "inliers": 0})
                            continue

                        k0 = f0['keypoints'].cpu().numpy()
                        s0 = f0['scores'].cpu().numpy().flatten() 

                        mkpts0, mkpts1, m_scores = model.match_lighterglue(f0, f1)
                        m_scores = m_scores.flatten() 

                        # mkpts0 = mkpts0.cpu().numpy()
                        # mkpts1 = mkpts1.cpu().numpy()
                        # m_scores = m_scores.cpu().numpy()

                        raw_matches = [RawMatch(KeypointData(m0[0], m0[1], s0[i]), 
                                       KeypointData(m1[0], m1[1], 1.0), ms) 
                                       for i, (m0, m1, ms) in enumerate(zip(mkpts0, mkpts1, m_scores))]
                        all_sat_kpts = [KeypointData(k[0], k[1], s) for k, s in zip(k0, s0)]
                    else:
                        f0, f1 = extractor({'image': img_s_t}), extractor({'image': img_d_t})
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è LightGlue
                        if f0['keypoints'].shape[1] == 0 or f1['keypoints'].shape[1] == 0:
                            print(f"\r  [{frame_idx}] ‚ö†Ô∏è {img_name} | –¢–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–ø—É—Å–∫...", end='')
                            model_frame_stats.append({"mAP": 0.0, "inliers": 0})
                            continue

                        k0, s0 = f0['keypoints'][0].cpu().numpy(), f0['keypoint_scores'][0].cpu().numpy()
                        res = matcher({'image0': f0, 'image1': f1})
                        idx = res['matches'][0].cpu().numpy()
                        m_scores = res['scores'][0].cpu().numpy() if 'scores' in res else [1.0]*len(idx)
                        raw_matches = [RawMatch(KeypointData(k0[i0][0], k0[i0][1], s0[i0]), 
                                                KeypointData(f1['keypoints'][0][i1][0].item(), f1['keypoints'][0][i1][1].item(), 1.0), 
                                                m_scores[i]) for i, (i0, i1) in enumerate(idx) if i0 != -1]
                        all_sat_kpts = [KeypointData(k[0], k[1], s) for k, s in zip(k0, s0)]

                inliers, _ = verifier.filter_matches(raw_matches)
                stats = metrics_calc.calculate_reliability_metrics(all_sat_kpts, inliers)
                model_frame_stats.append(stats)

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                self.visualizer.save_analysis_plot(img_s, img_d, all_sat_kpts, raw_matches, inliers, model_name, img_name)
                print(f"\r  [{frame_idx}] Processed {img_name} | Inliers: {len(inliers)}", end='')

            all_frames_data[model_name] = model_frame_stats
            if model_frame_stats:
                final_summary[model_name] = {
                    "mAP": np.mean([f['mAP'] for f in model_frame_stats]),
                    "Inliers": np.mean([f['inliers'] for f in model_frame_stats])
                }

            if model_name == "xfeat": del model
            else: del extractor, matcher
            gc.collect(); torch.cuda.empty_cache()

        self.visualizer.save_summary_statistics(all_frames_data)
        return final_summary

# ==========================================
# 5. –¢–û–ß–ö–ê –í–•–û–î–ê
# ==========================================

if __name__ == "__main__":
    from DataProvider import DataProvider # –ò–º–ø–æ—Ä—Ç –≤–∞—à–µ–≥–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª–∞—Å—Å–∞
    
    DATASET_PATH = "./" # –£–∫–∞–∂–∏—Ç–µ –≤–∞—à –ø—É—Ç—å
    # DATASET_PATH = "../Novorosia_dataset_sp_lg/data/401_080824_Novorossia/DCIM_1/"
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    loader = DataProvider(DATASET_PATH)
    verifier = GeometryVerifier(threshold=3.0)
    calculator = MetricsCalculator()
    benchmark = WaypointBenchmark(device=DEVICE, vis_scale=45)
    
    results = benchmark.execute(loader, verifier, calculator)
    
    print("\n\n" + "="*50)
    print(f"{'Algorithm':<15} | {'mAP':<10} | {'Avg Inliers'}")
    print("-" * 50)
    for model, res in results.items():
        print(f"{model.upper():<15} | {res['mAP']:<10.4f} | {res['Inliers']:.1f}")
    print("="*50)